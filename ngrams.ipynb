{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfSzZmVVg_f3"
   },
   "source": [
    "# Notebook for Finding Ngrams\n",
    "\n",
    "Using scikit-learn, we want to find ngrams (most commonly occuring sets of words) in the articles across each year.\n",
    "\n",
    "Currently processes the \"Fakespeak-ENG modified.xlsx\" file (I've renamed my copy to \"Fakespeak_ENG_modified.xlsx\" to create a more consistent path), but will eventually be run on data from MisInfoText as well.\n",
    "\n",
    "From the original data file, we use the following columns: ID, combinedLabel, originalTextType, originalBodyText, originalDateYear\n",
    "\n",
    "We are processing text from the \"originalBodyText\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3972,
     "status": "ok",
     "timestamp": 1734639663601,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "DZ6Pu-lugDAx"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from dataset_config import BASE_FAKESPEAK_CONFIG, BASE_MISINFOTEXT_CONFIG\n",
    "from helpers import get_groups, make_output_path, make_output_path_for_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFouFkvhwr2"
   },
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakespeak_config = BASE_FAKESPEAK_CONFIG | {\n",
    "    \"headline_col\": \"originalHeadline\",\n",
    "    \"usecols\": BASE_FAKESPEAK_CONFIG[\"usecols\"] + [\"originalHeadline\"]\n",
    "}\n",
    "\n",
    "misinfotext_config = BASE_MISINFOTEXT_CONFIG | {\n",
    "    \"headline_col\": \"originalHeadline\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_dataset = fakespeak_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4063,
     "status": "ok",
     "timestamp": 1734639714338,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "pETCTEaHiGSN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>combinedLabel</th>\n",
       "      <th>originalTextType</th>\n",
       "      <th>originalBodyText</th>\n",
       "      <th>originalHeadline</th>\n",
       "      <th>originalDateYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politifact_FALSE_Social media_687276</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Mexico is paying for the Wall through the new ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politifact_FALSE_Social media_25111</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Chuck Schumer: \"why should American citizens b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politifact_FALSE_Social media_735424</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Billions of dollars are sent to the State of C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politifact_FALSE_Social media_594307</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>If 50 Billion $$ were set aside to go towards ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politifact_FALSE_Social media_839325</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Huge@#CD 9 news. \\n@ncsbe\\n sent letter to eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID combinedLabel originalTextType  \\\n",
       "0  Politifact_FALSE_Social media_687276         False     Social media   \n",
       "1   Politifact_FALSE_Social media_25111         False     Social media   \n",
       "2  Politifact_FALSE_Social media_735424         False     Social media   \n",
       "3  Politifact_FALSE_Social media_594307         False     Social media   \n",
       "4  Politifact_FALSE_Social media_839325         False     Social media   \n",
       "\n",
       "                                    originalBodyText originalHeadline  \\\n",
       "0  Mexico is paying for the Wall through the new ...              NaN   \n",
       "1  Chuck Schumer: \"why should American citizens b...              NaN   \n",
       "2  Billions of dollars are sent to the State of C...              NaN   \n",
       "3  If 50 Billion $$ were set aside to go towards ...              NaN   \n",
       "4  Huge@#CD 9 news. \\n@ncsbe\\n sent letter to eve...              NaN   \n",
       "\n",
       "   originalDateYear  \n",
       "0              2019  \n",
       "1              2019  \n",
       "2              2019  \n",
       "3              2019  \n",
       "4              2019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_excel(\n",
    "    using_dataset[\"input_path\"], \n",
    "    sheet_name=using_dataset[\"sheet_name\"], \n",
    "    usecols=using_dataset[\"usecols\"]\n",
    ")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzY0GBDSk8bO"
   },
   "source": [
    "## Extracting ngrams\n",
    "\n",
    "Here we use sklearn's CountVectorizer() function to produce ngrams where n=1-5. First, we separate the fakespeak_df into its respective years, then find these ngrams and create new dataframes to hold them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1734642816321,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "HwHfyCtfnsy-"
   },
   "outputs": [],
   "source": [
    "# helper function to find ngrams for articles from each year\n",
    "def get_ngram_counts(df: pd.DataFrame, col: str):\n",
    "  # initialize vector\n",
    "  c_vec = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "  # input to fit_transform must be an iterable of strings\n",
    "  ngrams = c_vec.fit_transform(df[col].to_list())\n",
    "\n",
    "  # initialize vocabulary after calling fit_transform\n",
    "  vocab = c_vec.vocabulary_\n",
    "\n",
    "  count_values = ngrams.toarray().sum(axis=0)\n",
    "\n",
    "  # list to hold ngram rows that will be turned into a dataframe\n",
    "  ngram_list = []\n",
    "\n",
    "  for count, text in sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True):\n",
    "    n = len(text.split())\n",
    "    ngram_list.append([n, text, count])\n",
    "\n",
    "  headers = ['n', 'ngram_text', 'ngram_count']\n",
    "  ngram_df = pd.DataFrame(ngram_list, columns=headers)\n",
    "\n",
    "  # sort the dataframe by n\n",
    "  ngram_df = ngram_df.sort_values(by=['n', 'ngram_count'], ascending=[True, False])\n",
    "\n",
    "  return ngram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOVdIvRzR1G"
   },
   "source": [
    "## Prepare dataframes to output to spreadsheet\n",
    "Currently, the dataframes hold all found n-grams, including ones that are fully comprised of stop words. We are not interested in those, so we only keep n-grams that have meaningful content, and are not just stop words.\n",
    "\n",
    "Furthermore, we only take the first 20 entries for each n=2-5 (i.e. we take the first 20 bigrams, then the first 20 trigrams, etc. for each year). The exception is we take the first 50 monogram entries, since a lot of them tend to be common words and the results are more interesting when we broaden the search. These are the most common n-grams (since they are sorted descending by count), which are the ones most interesting to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1734645940563,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "w7FzGJgQ0ht_"
   },
   "outputs": [],
   "source": [
    "def is_all_stop_words(text: str):\n",
    "  tokens = word_tokenize(text)\n",
    "  return all(token in stop_words for token in tokens)\n",
    "\n",
    "# helper function that cleans up the dataframes as outlined above\n",
    "# where df is the ngram dataframe\n",
    "# num_mono is the number of entries to include for monograms\n",
    "# num_other is the number of entries to include for the other ngrams\n",
    "# drop index indicates the number of rows we want to drop from the top of the dataframe\n",
    "def clean_ngram(df: pd.DataFrame, num_mono=50, num_other=20, drop_index=10):\n",
    "  # Get rid of n-grams that are fully comprised of stop words\n",
    "  # df[\"doc\"] = list(nlp.pipe(df[\"ngram_text\"]))\n",
    "  df = df[~df[\"ngram_text\"].apply(is_all_stop_words)]\n",
    "\n",
    "  # filter dataframe by ngram frequency\n",
    "  df1 = df[df['n'] == 1].head(num_mono)\n",
    "  df2 = df[df['n'] == 2].head(num_other)\n",
    "  df3 = df[df['n'] == 3].head(num_other)\n",
    "  df4 = df[df['n'] == 4].head(num_other)\n",
    "  df5 = df[df['n'] == 5].head(num_other)\n",
    "\n",
    "  # concatenate the dataframes along the rows\n",
    "  output_df = pd.concat([df1, df2, df3, df4, df5], axis=0)\n",
    "\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_years_dfs(df: pd.DataFrame):\n",
    "    years, years_dfs = get_groups(df, using_dataset[\"year_col\"])\n",
    "    headline_years_df = [df[~df[using_dataset[\"headline_col\"]].isna()] for df in years_dfs]\n",
    "    \n",
    "    ngrams_text_years_dfs = [clean_ngram(get_ngram_counts(df, using_dataset[\"text_col\"])) for df in years_dfs]\n",
    "    ngrams_headline_years_dfs = [clean_ngram(get_ngram_counts(df, using_dataset[\"headline_col\"])) for df in headline_years_df]\n",
    "    \n",
    "    return years, ngrams_text_years_dfs, ngrams_headline_years_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "years, ngrams_text_years_dfs, ngrams_headline_years_dfs = get_ngram_years_dfs(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>trump</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>people</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>gun</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>president</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n ngram_text  ngram_count\n",
       "29  1      trump          111\n",
       "40  1     people           85\n",
       "43  1        one           79\n",
       "45  1        gun           73\n",
       "53  1  president           65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_text_years_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>trump</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>new</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>deal</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>border</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>report</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n ngram_text  ngram_count\n",
       "5   1      trump            4\n",
       "7   1        new            4\n",
       "8   1       deal            4\n",
       "9   1     border            4\n",
       "10  1     report            3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_headline_years_dfs[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBMhLeeD3s9r"
   },
   "source": [
    "We could show the resulting dataframes for the other years as well, but here I've chosen not to in order to save space and improve readability for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJUd0Gyh3rpe"
   },
   "source": [
    "## Write dataframes to excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = make_output_path(using_dataset, \"ngrams\")\n",
    "\n",
    "writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "for year, df in zip(years, ngrams_text_years_dfs):\n",
    "    df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = make_output_path(using_dataset, \"ngrams_headlines\")\n",
    "\n",
    "writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "for year, df in zip(years, ngrams_headline_years_dfs):\n",
    "    df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the same analysis for each separate text type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "types, types_dfs = get_groups(dataset_df, using_dataset[\"type_col\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type, df in zip(types, types_dfs):\n",
    "    years, ngrams_text_years_dfs, ngrams_headline_years_dfs = get_ngram_years_dfs(dataset_df)\n",
    "\n",
    "    output_path = make_output_path_for_type(using_dataset, type, \"ngrams\")\n",
    "\n",
    "    writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    for year, df in zip(years, ngrams_text_years_dfs):\n",
    "        df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    output_path = make_output_path_for_type(using_dataset, type, \"ngrams_headlines\")\n",
    "\n",
    "    writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    for year, df in zip(years, ngrams_headline_years_dfs):\n",
    "        df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7OonnWjvEWZsKD567Kb1t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
