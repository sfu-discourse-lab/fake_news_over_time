{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfSzZmVVg_f3"
   },
   "source": [
    "# Notebook for Finding Ngrams\n",
    "\n",
    "Using scikit-learn, we want to find ngrams (most commonly occuring sets of words) in the articles across each year.\n",
    "\n",
    "Currently processes the \"Fakespeak-ENG modified.xlsx\" file (I've renamed my copy to \"Fakespeak_ENG_modified.xlsx\" to create a more consistent path), but will eventually be run on data from MisInfoText as well.\n",
    "\n",
    "From the original data file, we use the following columns: ID, combinedLabel, originalTextType, originalBodyText, originalDateYear\n",
    "\n",
    "We are processing text from the \"originalBodyText\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3972,
     "status": "ok",
     "timestamp": 1734639663601,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "DZ6Pu-lugDAx"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1734639685620,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "M96g3OTciEQO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18372,
     "status": "ok",
     "timestamp": 1734639705185,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "MhB8SY_NhtDW",
    "outputId": "f263dea7-611c-498f-d6aa-393f1cb322a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFouFkvhwr2"
   },
   "source": [
    "## Load in fakespeak dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1734639709298,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "jeJqAoaRhzqe"
   },
   "outputs": [],
   "source": [
    "# file path for fakespeak excel sheet\n",
    "# input = '/content/drive/My Drive/fake_news_over_time/Fakespeak_ENG_modified.xlsx'\n",
    "input = \"./data/Fakespeak-ENG/Fakespeak-ENG modified.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4063,
     "status": "ok",
     "timestamp": 1734639714338,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "pETCTEaHiGSN"
   },
   "outputs": [],
   "source": [
    "fakespeak_df = pd.read_excel(input, sheet_name=\"Working\", usecols=['ID', 'combinedLabel', 'originalTextType', 'originalBodyText', 'originalDateYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734639714338,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "ZwIALTveiRbW",
    "outputId": "07c7900f-fee7-4b5b-f94b-1586fdea8b21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>combinedLabel</th>\n",
       "      <th>originalTextType</th>\n",
       "      <th>originalBodyText</th>\n",
       "      <th>originalDateYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politifact_FALSE_Social media_687276</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Mexico is paying for the Wall through the new ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politifact_FALSE_Social media_25111</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Chuck Schumer: \"why should American citizens b...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politifact_FALSE_Social media_735424</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Billions of dollars are sent to the State of C...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politifact_FALSE_Social media_594307</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>If 50 Billion $$ were set aside to go towards ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politifact_FALSE_Social media_839325</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Huge@#CD 9 news. \\n@ncsbe\\n sent letter to eve...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID combinedLabel originalTextType  \\\n",
       "0  Politifact_FALSE_Social media_687276         False     Social media   \n",
       "1   Politifact_FALSE_Social media_25111         False     Social media   \n",
       "2  Politifact_FALSE_Social media_735424         False     Social media   \n",
       "3  Politifact_FALSE_Social media_594307         False     Social media   \n",
       "4  Politifact_FALSE_Social media_839325         False     Social media   \n",
       "\n",
       "                                    originalBodyText  originalDateYear  \n",
       "0  Mexico is paying for the Wall through the new ...              2019  \n",
       "1  Chuck Schumer: \"why should American citizens b...              2019  \n",
       "2  Billions of dollars are sent to the State of C...              2019  \n",
       "3  If 50 Billion $$ were set aside to go towards ...              2019  \n",
       "4  Huge@#CD 9 news. \\n@ncsbe\\n sent letter to eve...              2019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakespeak_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzY0GBDSk8bO"
   },
   "source": [
    "## Extracting ngrams\n",
    "\n",
    "Here we use sklearn's CountVectorizer() function to produce ngrams where n=1-5. First, we separate the fakespeak_df into its respective years, then find these ngrams and create new dataframes to hold them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1734642816321,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "HwHfyCtfnsy-"
   },
   "outputs": [],
   "source": [
    "# helper function to find ngrams for articles from each year\n",
    "def vectorize(df, year):\n",
    "  # filter the dataframe\n",
    "  year_df = df[df['originalDateYear'] == year]\n",
    "\n",
    "  # initialize vector\n",
    "  c_vec = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "  # input to fit_transform must be an iterable of strings\n",
    "  ngrams = c_vec.fit_transform(year_df['originalBodyText'].to_list())\n",
    "\n",
    "  # initialize vocabulary after calling fit_transform\n",
    "  vocab = c_vec.vocabulary_\n",
    "\n",
    "  count_values = ngrams.toarray().sum(axis=0)\n",
    "\n",
    "  # list to hold ngram rows that will be turned into a dataframe\n",
    "  ngram_list = []\n",
    "\n",
    "  for count, text in sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True):\n",
    "    n = len(text.split())\n",
    "    ngram_list.append([n, text, count])\n",
    "\n",
    "  headers = ['n', 'ngram_text', 'ngram_count']\n",
    "  ngram_df = pd.DataFrame(ngram_list, columns=headers)\n",
    "\n",
    "  # sort the dataframe by n\n",
    "  ngram_df = ngram_df.sort_values(by=['n', 'ngram_count'], ascending=[True, False])\n",
    "\n",
    "  return ngram_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 28824,
     "status": "ok",
     "timestamp": 1734644387254,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "aRFk3uJMk_O2"
   },
   "outputs": [],
   "source": [
    "# get ngrams for each year\n",
    "ngram_19_df = vectorize(fakespeak_df, 2019)\n",
    "ngram_20_df = vectorize(fakespeak_df, 2020)\n",
    "ngram_21_df = vectorize(fakespeak_df, 2021)\n",
    "ngram_22_df = vectorize(fakespeak_df, 2022)\n",
    "ngram_23_df = vectorize(fakespeak_df, 2023)\n",
    "ngram_24_df = vectorize(fakespeak_df, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOVdIvRzR1G"
   },
   "source": [
    "## Prepare dataframes to output to spreadsheet\n",
    "Currently, the dataframes hold all found ngrams with n=1-5, including ones that only appear once (which isn't very helpful - for reference, the unfiltered 2019 dataframe contains 115,090 entries). To address this issue, we only take the first 20 entries for each n=2-5 (i.e. we take the first 20 bigrams, then the first 20 trigrams, etc. for each year).\n",
    "\n",
    "The exception is we take the first 50 monogram entries, since a lot of them tend to be common words and the results are more interesting when we broaden the search. To circumvent this, we also drop the first 10 rows from each dataframe to go further down the monogram list (this can also be adjusted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1734645940563,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "w7FzGJgQ0ht_"
   },
   "outputs": [],
   "source": [
    "# helper function that cleans up the dataframes as outlined above\n",
    "# where df is the ngram dataframe\n",
    "# num_mono is the number of entries to include for monograms\n",
    "# num_other is the number of entries to include for the other ngrams\n",
    "# drop index indicates the number of rows we want to drop from the top of the dataframe\n",
    "def clean_ngram(df, num_mono, num_other, drop_index):\n",
    "  # drop 20 most common ngrams\n",
    "  df = df.iloc[drop_index:]\n",
    "\n",
    "  # filter dataframe by ngram frequency\n",
    "  df1 = df[df['n'] == 1].head(num_mono)\n",
    "  df2 = df[df['n'] == 2].head(num_other)\n",
    "  df3 = df[df['n'] == 3].head(num_other)\n",
    "  df4 = df[df['n'] == 4].head(num_other)\n",
    "  df5 = df[df['n'] == 5].head(num_other)\n",
    "\n",
    "  # concatenate the dataframes along the rows\n",
    "  output_df = pd.concat([df1, df2, df3, df4, df5], axis=0)\n",
    "\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1734645943668,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "TVPwLXRrq8TW"
   },
   "outputs": [],
   "source": [
    "num_entries_mono = 50\n",
    "num_entries_other = 20\n",
    "drop_index = 10\n",
    "\n",
    "clean_ngram_19_df = clean_ngram(ngram_19_df, num_entries_mono, num_entries_other, drop_index)\n",
    "clean_ngram_20_df = clean_ngram(ngram_20_df, num_entries_mono, num_entries_other, drop_index)\n",
    "clean_ngram_21_df = clean_ngram(ngram_21_df, num_entries_mono, num_entries_other, drop_index)\n",
    "clean_ngram_22_df = clean_ngram(ngram_22_df, num_entries_mono, num_entries_other, drop_index)\n",
    "clean_ngram_23_df = clean_ngram(ngram_23_df, num_entries_mono, num_entries_other, drop_index)\n",
    "clean_ngram_24_df = clean_ngram(ngram_24_df, num_entries_mono, num_entries_other, drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1734645962792,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "fhBqJxSt2y0-",
    "outputId": "1183da9d-e575-42fc-f927-cc2ec0281f38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>we</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>are</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>you</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>with</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n ngram_text  ngram_count\n",
       "10  1         on          221\n",
       "11  1         we          217\n",
       "12  1        are          208\n",
       "13  1        you          196\n",
       "14  1       with          189"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ngram_19_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1734645961218,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "w4_nGgCx3mZW",
    "outputId": "1369c971-6d39-4c75-a5e3-0840db8f4c0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>are</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>you</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>be</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>with</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>5</td>\n",
       "      <td>live in nation that was</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>5</td>\n",
       "      <td>is run by idiots if</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>5</td>\n",
       "      <td>in nation that was founded</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>5</td>\n",
       "      <td>related biological products advisory committee</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>5</td>\n",
       "      <td>and related biological products advisory</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n                                      ngram_text  ngram_count\n",
       "10    1                                              on          850\n",
       "11    1                                             are          730\n",
       "12    1                                             you          695\n",
       "13    1                                              be          674\n",
       "14    1                                            with          668\n",
       "...  ..                                             ...          ...\n",
       "2435  5                         live in nation that was           10\n",
       "2458  5                             is run by idiots if           10\n",
       "2469  5                      in nation that was founded           10\n",
       "2677  5  related biological products advisory committee            9\n",
       "2908  5        and related biological products advisory            9\n",
       "\n",
       "[130 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ngram_20_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBMhLeeD3s9r"
   },
   "source": [
    "We could show the resulting dataframes for the other years as well, but here I've chosen not to in order to save space and improve readability for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJUd0Gyh3rpe"
   },
   "source": [
    "## Write dataframes to excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st2GB47-4AAN"
   },
   "outputs": [],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1734645977614,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "459H4GrD312e"
   },
   "outputs": [],
   "source": [
    "# file path for output excel spreadsheet\n",
    "output = '/content/drive/My Drive/fake_news_over_time/ngrams.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1734645980648,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "vYC-AUPt4H5-"
   },
   "outputs": [],
   "source": [
    "# create excel writer object to initialize new workbook\n",
    "writer = pd.ExcelWriter(output, engine=\"xlsxwriter\")\n",
    "\n",
    "# write dataframes to different worksheets\n",
    "clean_ngram_19_df.to_excel(writer, sheet_name=\"2019\", index=False)\n",
    "clean_ngram_20_df.to_excel(writer, sheet_name=\"2020\", index=False)\n",
    "clean_ngram_21_df.to_excel(writer, sheet_name=\"2021\", index=False)\n",
    "clean_ngram_22_df.to_excel(writer, sheet_name=\"2022\", index=False)\n",
    "clean_ngram_23_df.to_excel(writer, sheet_name=\"2023\", index=False)\n",
    "clean_ngram_24_df.to_excel(writer, sheet_name=\"2024\", index=False)\n",
    "\n",
    "# close the excel writer and output file\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7OonnWjvEWZsKD567Kb1t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
