{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfSzZmVVg_f3"
   },
   "source": [
    "# Notebook for Finding Ngrams\n",
    "\n",
    "Using scikit-learn, we want to find ngrams (most commonly occuring sets of words) in the articles across each year.\n",
    "\n",
    "Currently processes the \"Fakespeak-ENG modified.xlsx\" file (I've renamed my copy to \"Fakespeak_ENG_modified.xlsx\" to create a more consistent path), but will eventually be run on data from MisInfoText as well.\n",
    "\n",
    "From the original data file, we use the following columns: ID, combinedLabel, originalTextType, originalBodyText, originalDateYear\n",
    "\n",
    "We are processing text from the \"originalBodyText\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3972,
     "status": "ok",
     "timestamp": 1734639663601,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "DZ6Pu-lugDAx"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from dataset_config import BASE_FAKESPEAK_CONFIG, BASE_MISINFOTEXT_CONFIG\n",
    "from helpers import get_groups, make_output_path, make_output_path_for_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFouFkvhwr2"
   },
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakespeak_config = BASE_FAKESPEAK_CONFIG | {\n",
    "    \"headline_col\": \"originalHeadline\",\n",
    "    \"usecols\": BASE_FAKESPEAK_CONFIG[\"usecols\"] + [\"originalHeadline\"]\n",
    "}\n",
    "\n",
    "misinfotext_config = BASE_MISINFOTEXT_CONFIG | {\n",
    "    \"headline_col\": \"originalHeadline\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_dataset = misinfotext_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4063,
     "status": "ok",
     "timestamp": 1734639714338,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "pETCTEaHiGSN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factcheckURL</th>\n",
       "      <th>originalURL</th>\n",
       "      <th>originalBodyText</th>\n",
       "      <th>originalHeadline</th>\n",
       "      <th>originalTextType</th>\n",
       "      <th>originalDate</th>\n",
       "      <th>originalDateYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.politifact.com/arizona/statements/2...</td>\n",
       "      <td>https://associatedmediacoverage.com/three-stat...</td>\n",
       "      <td>Residents of multiple states will be asked to ...</td>\n",
       "      <td>Multiple States Have Agreed To Implement A ‘Tw...</td>\n",
       "      <td>News and blog</td>\n",
       "      <td>2016-05-06</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.politifact.com/california/statement...</td>\n",
       "      <td>https://users.focalbeam.com/fs/distribution:wl...</td>\n",
       "      <td>Sacramento, CA - United States Senator Dianne ...</td>\n",
       "      <td>U.S. Senator Dianne Feinstein Opposes Prop. 64...</td>\n",
       "      <td>Press release</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.politifact.com/california/statement...</td>\n",
       "      <td>http://www.sacbee.com/opinion/op-ed/soapbox/ar...</td>\n",
       "      <td>We should anticipate black and gray markets in...</td>\n",
       "      <td>Why you should buy a locking gasoline cap</td>\n",
       "      <td>News and blog</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.politifact.com/california/statement...</td>\n",
       "      <td>https://nocagastax.com/california-gas-tax-hike...</td>\n",
       "      <td>As a ballot initiative calling for repeal of a...</td>\n",
       "      <td>California Gas-Tax-Hike Repeal Campaign Heats Up</td>\n",
       "      <td>News and blog</td>\n",
       "      <td>2017-06-15</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.politifact.com/california/statement...</td>\n",
       "      <td>https://chu.house.gov/media-center/press-relea...</td>\n",
       "      <td>WASHINGTON, DC  The House of Representatives t...</td>\n",
       "      <td>Rep. Chu Decries \"Heartless\" ACA Repeal Vote</td>\n",
       "      <td>Press release</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        factcheckURL  \\\n",
       "0  http://www.politifact.com/arizona/statements/2...   \n",
       "1  http://www.politifact.com/california/statement...   \n",
       "2  http://www.politifact.com/california/statement...   \n",
       "3  http://www.politifact.com/california/statement...   \n",
       "4  http://www.politifact.com/california/statement...   \n",
       "\n",
       "                                         originalURL  \\\n",
       "0  https://associatedmediacoverage.com/three-stat...   \n",
       "1  https://users.focalbeam.com/fs/distribution:wl...   \n",
       "2  http://www.sacbee.com/opinion/op-ed/soapbox/ar...   \n",
       "3  https://nocagastax.com/california-gas-tax-hike...   \n",
       "4  https://chu.house.gov/media-center/press-relea...   \n",
       "\n",
       "                                    originalBodyText  \\\n",
       "0  Residents of multiple states will be asked to ...   \n",
       "1  Sacramento, CA - United States Senator Dianne ...   \n",
       "2  We should anticipate black and gray markets in...   \n",
       "3  As a ballot initiative calling for repeal of a...   \n",
       "4  WASHINGTON, DC  The House of Representatives t...   \n",
       "\n",
       "                                    originalHeadline originalTextType  \\\n",
       "0  Multiple States Have Agreed To Implement A ‘Tw...    News and blog   \n",
       "1  U.S. Senator Dianne Feinstein Opposes Prop. 64...    Press release   \n",
       "2          Why you should buy a locking gasoline cap    News and blog   \n",
       "3   California Gas-Tax-Hike Repeal Campaign Heats Up    News and blog   \n",
       "4       Rep. Chu Decries \"Heartless\" ACA Repeal Vote    Press release   \n",
       "\n",
       "  originalDate  originalDateYear  \n",
       "0   2016-05-06              2016  \n",
       "1   2016-07-12              2016  \n",
       "2   2017-08-04              2017  \n",
       "3   2017-06-15              2017  \n",
       "4   2017-05-04              2017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_excel(\n",
    "    using_dataset[\"input_path\"], \n",
    "    sheet_name=using_dataset[\"sheet_name\"], \n",
    "    usecols=using_dataset[\"usecols\"]\n",
    ")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzY0GBDSk8bO"
   },
   "source": [
    "## Extracting ngrams\n",
    "\n",
    "Here we use sklearn's CountVectorizer() function to produce ngrams where n=1-5. First, we separate the fakespeak_df into its respective years, then find these ngrams and create new dataframes to hold them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1734642816321,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "HwHfyCtfnsy-"
   },
   "outputs": [],
   "source": [
    "# helper function to find ngrams for articles from each year\n",
    "def get_ngram_counts(df: pd.DataFrame, col: str):\n",
    "  try:\n",
    "    # initialize vector\n",
    "    c_vec = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "    # input to fit_transform must be an iterable of strings\n",
    "    ngrams = c_vec.fit_transform(df[col].to_list())\n",
    "\n",
    "    # initialize vocabulary after calling fit_transform\n",
    "    vocab = c_vec.vocabulary_\n",
    "\n",
    "    count_values = ngrams.toarray().sum(axis=0)\n",
    "\n",
    "    # list to hold ngram rows that will be turned into a dataframe\n",
    "    ngram_list = []\n",
    "\n",
    "    for count, text in sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True):\n",
    "      n = len(text.split())\n",
    "      ngram_list.append([n, text, count])\n",
    "\n",
    "    headers = ['n', 'ngram_text', 'ngram_count']\n",
    "    ngram_df = pd.DataFrame(ngram_list, columns=headers)\n",
    "\n",
    "    # sort the dataframe by n\n",
    "    ngram_df = ngram_df.sort_values(by=['n', 'ngram_count'], ascending=[True, False])\n",
    "\n",
    "    return ngram_df\n",
    "  except Exception as e:\n",
    "    print(\"Error getting ngram counts\")\n",
    "    print(e)\n",
    "\n",
    "    # Return empty dataframe\n",
    "    return pd.DataFrame({\n",
    "      \"n\": [],\n",
    "      \"ngram_text\": [],\n",
    "      \"ngram_count\": [],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOVdIvRzR1G"
   },
   "source": [
    "## Prepare dataframes to output to spreadsheet\n",
    "Currently, the dataframes hold all found n-grams, including ones that are fully comprised of stop words. We are not interested in those, so we only keep n-grams that have meaningful content, and are not just stop words.\n",
    "\n",
    "Furthermore, we only take the first 20 entries for each n=2-5 (i.e. we take the first 20 bigrams, then the first 20 trigrams, etc. for each year). The exception is we take the first 50 monogram entries, since a lot of them tend to be common words and the results are more interesting when we broaden the search. These are the most common n-grams (since they are sorted descending by count), which are the ones most interesting to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1734645940563,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "w7FzGJgQ0ht_"
   },
   "outputs": [],
   "source": [
    "def is_all_stop_words(text: str):\n",
    "  tokens = word_tokenize(text)\n",
    "  return all(token in stop_words for token in tokens)\n",
    "\n",
    "# helper function that cleans up the dataframes as outlined above\n",
    "# where df is the ngram dataframe\n",
    "# num_mono is the number of entries to include for monograms\n",
    "# num_other is the number of entries to include for the other ngrams\n",
    "# drop index indicates the number of rows we want to drop from the top of the dataframe\n",
    "def clean_ngram(df: pd.DataFrame, num_mono=50, num_other=20, drop_index=10):\n",
    "  # Get rid of n-grams that are fully comprised of stop words\n",
    "  # df[\"doc\"] = list(nlp.pipe(df[\"ngram_text\"]))\n",
    "\n",
    "  # Pass through the empty df\n",
    "  if df.shape[0] == 0:\n",
    "    return df\n",
    "\n",
    "  df = df[~df[\"ngram_text\"].apply(is_all_stop_words)]\n",
    "\n",
    "  # filter dataframe by ngram frequency\n",
    "  df1 = df[df['n'] == 1].head(num_mono)\n",
    "  df2 = df[df['n'] == 2].head(num_other)\n",
    "  df3 = df[df['n'] == 3].head(num_other)\n",
    "  df4 = df[df['n'] == 4].head(num_other)\n",
    "  df5 = df[df['n'] == 5].head(num_other)\n",
    "\n",
    "  # concatenate the dataframes along the rows\n",
    "  output_df = pd.concat([df1, df2, df3, df4, df5], axis=0)\n",
    "\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_years_dfs(df: pd.DataFrame):\n",
    "    years, years_dfs = get_groups(df, using_dataset[\"year_col\"])\n",
    "    headline_years_df = [df[~df[using_dataset[\"headline_col\"]].isna()] for df in years_dfs]\n",
    "    \n",
    "    ngrams_text_years_dfs = [clean_ngram(get_ngram_counts(df, using_dataset[\"text_col\"])) for df in years_dfs]\n",
    "    ngrams_headline_years_dfs = [clean_ngram(get_ngram_counts(df, using_dataset[\"headline_col\"])) for df in headline_years_df]\n",
    "    \n",
    "    return years, ngrams_text_years_dfs, ngrams_headline_years_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "years, ngrams_text_years_dfs, ngrams_headline_years_dfs = get_ngram_years_dfs(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>troops</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>senator</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>surge</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>new</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n  ngram_text  ngram_count\n",
       "4   1      troops            3\n",
       "6   1     senator            3\n",
       "11  1  withdrawal            2\n",
       "17  1       surge            2\n",
       "20  1         new            2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_text_years_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>statement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>mccain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>hillary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>clinton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n ngram_text  ngram_count\n",
       "4   1  statement            1\n",
       "11  1     mccain            1\n",
       "16  1       john            1\n",
       "18  1    hillary            1\n",
       "19  1    clinton            1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_headline_years_dfs[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBMhLeeD3s9r"
   },
   "source": [
    "We could show the resulting dataframes for the other years as well, but here I've chosen not to in order to save space and improve readability for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJUd0Gyh3rpe"
   },
   "source": [
    "## Write dataframes to excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = make_output_path(using_dataset, \"ngrams\")\n",
    "\n",
    "writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "for year, df in zip(years, ngrams_text_years_dfs):\n",
    "    df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = make_output_path(using_dataset, \"ngrams_headlines\")\n",
    "\n",
    "writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "for year, df in zip(years, ngrams_headline_years_dfs):\n",
    "    df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the same analysis for each separate text type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "types, types_dfs = get_groups(dataset_df, using_dataset[\"type_col\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting ngram counts\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "Error getting ngram counts\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "Error getting ngram counts\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "Error getting ngram counts\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "Error getting ngram counts\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "Error getting ngram counts\n",
      "empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    }
   ],
   "source": [
    "for type, df in zip(types, types_dfs):\n",
    "    years, ngrams_text_years_dfs, ngrams_headline_years_dfs = get_ngram_years_dfs(df)\n",
    "\n",
    "    output_path = make_output_path_for_type(using_dataset, type, \"ngrams\")\n",
    "\n",
    "    writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    for year, df in zip(years, ngrams_text_years_dfs):\n",
    "        df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    output_path = make_output_path_for_type(using_dataset, type, \"ngrams_headlines\")\n",
    "\n",
    "    writer = pd.ExcelWriter(output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    for year, df in zip(years, ngrams_headline_years_dfs):\n",
    "        df.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7OonnWjvEWZsKD567Kb1t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
