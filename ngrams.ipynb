{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfSzZmVVg_f3"
   },
   "source": [
    "# Notebook for Finding Ngrams\n",
    "\n",
    "Using scikit-learn, we want to find ngrams (most commonly occuring sets of words) in the articles across each year.\n",
    "\n",
    "Currently processes the \"Fakespeak-ENG modified.xlsx\" file (I've renamed my copy to \"Fakespeak_ENG_modified.xlsx\" to create a more consistent path), but will eventually be run on data from MisInfoText as well.\n",
    "\n",
    "From the original data file, we use the following columns: ID, combinedLabel, originalTextType, originalBodyText, originalDateYear\n",
    "\n",
    "We are processing text from the \"originalBodyText\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3972,
     "status": "ok",
     "timestamp": 1734639663601,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "DZ6Pu-lugDAx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from dataset_config import BASE_FAKESPEAK_CONFIG, BASE_MISINFOTEXT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18372,
     "status": "ok",
     "timestamp": 1734639705185,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "MhB8SY_NhtDW",
    "outputId": "f263dea7-611c-498f-d6aa-393f1cb322a1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFouFkvhwr2"
   },
   "source": [
    "## Load in fakespeak dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out how to get this modular system working for headlines too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakespeak_config = BASE_FAKESPEAK_CONFIG | {\n",
    "    \"output_path\": \"./data/Fakespeak-ENG/Analysis_output/Fakespeak_ngrams.xlsx\",\n",
    "    \"output_path_headlines\": \"./data/Fakespeak-ENG/Analysis_output/Fakespeak_ngrams_headlines.xlsx\",\n",
    "    \"usecols\": BASE_FAKESPEAK_CONFIG[\"usecols\"] + [\"originalHeadline\"]\n",
    "}\n",
    "\n",
    "misinfotext_config = BASE_MISINFOTEXT_CONFIG | {\n",
    "    \"output_path\": \"./data/MisInfoText/Analysis_output/MisInfoText_ngrams_headlines.xlsx\",\n",
    "    \"output_path_headlines\": \"./data/MisInfoText/Analysis_output/MisInfoText_ngrams_headlines.xlsx\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_dataset = fakespeak_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4063,
     "status": "ok",
     "timestamp": 1734639714338,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "pETCTEaHiGSN"
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.read_excel(\n",
    "    using_dataset[\"input_path\"], \n",
    "    sheet_name=using_dataset[\"sheet_name\"], \n",
    "    usecols=using_dataset[\"usecols\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you want to filter by only\n",
    "# \"News and blog\" or \"Social media\" article types.\n",
    "# It will save to a separate subdirectory without overwriting\n",
    "# the existing files.\n",
    "only_use_news_blog_and_social_media = True\n",
    "\n",
    "if only_use_news_blog_and_social_media:\n",
    "    dataset_df = dataset_df[(dataset_df[\"originalTextType\"] == \"News and blog\") | (dataset_df[\"originalTextType\"] == \"Social media\")]\n",
    "    \n",
    "    output_path = using_dataset.output_path\n",
    "    output_path_split = output_path.split(\"/\")\n",
    "    output_path_split.insert(len(output_path_split) - 1, \"news_blog_and_social_media\")\n",
    "    using_dataset.output_path = \"/\".join(output_path_split)\n",
    "\n",
    "    output_headlines_path = using_dataset.output_path_headlines\n",
    "    output_headlines_path_split = output_headlines_path.split(\"/\")\n",
    "    output_headlines_path_split.insert(len(output_headlines_path_split) - 1, \"news_blog_and_social_media\")\n",
    "    using_dataset.output_path_headlines = \"/\".join(output_headlines_path_split)\n",
    "\n",
    "    os.makedirs(\"/\".join(output_path_split[:-1]), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>combinedLabel</th>\n",
       "      <th>originalTextType</th>\n",
       "      <th>originalBodyText</th>\n",
       "      <th>originalHeadline</th>\n",
       "      <th>originalDateYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politifact_FALSE_Social media_687276</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Mexico is paying for the Wall through the new ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politifact_FALSE_Social media_25111</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Chuck Schumer: \"why should American citizens b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politifact_FALSE_Social media_735424</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Billions of dollars are sent to the State of C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politifact_FALSE_Social media_594307</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>If 50 Billion $$ were set aside to go towards ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politifact_FALSE_Social media_839325</td>\n",
       "      <td>False</td>\n",
       "      <td>Social media</td>\n",
       "      <td>Huge@#CD 9 news. \\n@ncsbe\\n sent letter to eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID combinedLabel originalTextType  \\\n",
       "0  Politifact_FALSE_Social media_687276         False     Social media   \n",
       "1   Politifact_FALSE_Social media_25111         False     Social media   \n",
       "2  Politifact_FALSE_Social media_735424         False     Social media   \n",
       "3  Politifact_FALSE_Social media_594307         False     Social media   \n",
       "4  Politifact_FALSE_Social media_839325         False     Social media   \n",
       "\n",
       "                                    originalBodyText originalHeadline  \\\n",
       "0  Mexico is paying for the Wall through the new ...              NaN   \n",
       "1  Chuck Schumer: \"why should American citizens b...              NaN   \n",
       "2  Billions of dollars are sent to the State of C...              NaN   \n",
       "3  If 50 Billion $$ were set aside to go towards ...              NaN   \n",
       "4  Huge@#CD 9 news. \\n@ncsbe\\n sent letter to eve...              NaN   \n",
       "\n",
       "   originalDateYear  \n",
       "0              2019  \n",
       "1              2019  \n",
       "2              2019  \n",
       "3              2019  \n",
       "4              2019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzY0GBDSk8bO"
   },
   "source": [
    "## Extracting ngrams\n",
    "\n",
    "Here we use sklearn's CountVectorizer() function to produce ngrams where n=1-5. First, we separate the fakespeak_df into its respective years, then find these ngrams and create new dataframes to hold them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1734642816321,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "HwHfyCtfnsy-"
   },
   "outputs": [],
   "source": [
    "# helper function to find ngrams for articles from each year\n",
    "def get_ngram_counts(df: pd.DataFrame, col: str):\n",
    "  year = df[using_dataset[\"year_col\"]].iloc[0]\n",
    "\n",
    "  # initialize vector\n",
    "  c_vec = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "  # input to fit_transform must be an iterable of strings\n",
    "  ngrams = c_vec.fit_transform(df[col].to_list())\n",
    "\n",
    "  # initialize vocabulary after calling fit_transform\n",
    "  vocab = c_vec.vocabulary_\n",
    "\n",
    "  count_values = ngrams.toarray().sum(axis=0)\n",
    "\n",
    "  # list to hold ngram rows that will be turned into a dataframe\n",
    "  ngram_list = []\n",
    "\n",
    "  for count, text in sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True):\n",
    "    n = len(text.split())\n",
    "    ngram_list.append([n, text, count, year])\n",
    "\n",
    "  headers = ['n', 'ngram_text', 'ngram_count', 'year']\n",
    "  ngram_df = pd.DataFrame(ngram_list, columns=headers)\n",
    "\n",
    "  # sort the dataframe by n\n",
    "  ngram_df = ngram_df.sort_values(by=['n', 'ngram_count'], ascending=[True, False])\n",
    "\n",
    "  return ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28824,
     "status": "ok",
     "timestamp": 1734644387254,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "aRFk3uJMk_O2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>1258</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>796</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>606</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>605</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>456</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92958</th>\n",
       "      <td>5</td>\n",
       "      <td>000 american citizens waiting for</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92962</th>\n",
       "      <td>5</td>\n",
       "      <td>000 000 that have been</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92966</th>\n",
       "      <td>5</td>\n",
       "      <td>00 to the trump campaign</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92970</th>\n",
       "      <td>5</td>\n",
       "      <td>00 ticket and you will</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92974</th>\n",
       "      <td>5</td>\n",
       "      <td>00 they have not posted</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92978 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n                         ngram_text  ngram_count  year\n",
       "0      1                                the         1258  2019\n",
       "1      1                                 to          796  2019\n",
       "2      1                                and          606  2019\n",
       "3      1                                 of          605  2019\n",
       "4      1                                 in          456  2019\n",
       "...   ..                                ...          ...   ...\n",
       "92958  5  000 american citizens waiting for            1  2019\n",
       "92962  5             000 000 that have been            1  2019\n",
       "92966  5           00 to the trump campaign            1  2019\n",
       "92970  5             00 ticket and you will            1  2019\n",
       "92974  5            00 they have not posted            1  2019\n",
       "\n",
       "[92978 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ngrams for each year\n",
    "grouped_by_year = dataset_df.groupby(by=\"originalDateYear\")\n",
    "ngram_years_dfs = [get_ngram_counts(grouped_by_year.get_group(group), \"originalBodyText\") \n",
    "                   for group in grouped_by_year.groups]\n",
    "ngram_years_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>trump</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>report</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>5</td>\n",
       "      <td>actor tom selleck would say</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>5</td>\n",
       "      <td>accuse trump of sexual attack</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>5</td>\n",
       "      <td>83 million left to trump</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>5</td>\n",
       "      <td>40 years in prison or</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>5</td>\n",
       "      <td>250 scientists warn emf from</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n                     ngram_text  ngram_count  year\n",
       "0     1                             to           12  2019\n",
       "1     1                            the            4  2019\n",
       "2     1                             in            4  2019\n",
       "3     1                          trump            3  2019\n",
       "4     1                         report            3  2019\n",
       "...  ..                            ...          ...   ...\n",
       "1045  5    actor tom selleck would say            1  2019\n",
       "1050  5  accuse trump of sexual attack            1  2019\n",
       "1057  5       83 million left to trump            1  2019\n",
       "1062  5          40 years in prison or            1  2019\n",
       "1067  5   250 scientists warn emf from            1  2019\n",
       "\n",
       "[1073 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_headlines = dataset_df[dataset_df[\"originalHeadline\"].notna()].copy()\n",
    "grouped_by_year_headlines = df_for_headlines.groupby(by=\"originalDateYear\")\n",
    "ngram_years_headlines_dfs = [get_ngram_counts(grouped_by_year_headlines.get_group(group), \"originalHeadline\") \n",
    "                             for group in grouped_by_year_headlines.groups]\n",
    "ngram_years_headlines_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOVdIvRzR1G"
   },
   "source": [
    "## Prepare dataframes to output to spreadsheet\n",
    "Currently, the dataframes hold all found ngrams with n=1-5, including ones that only appear once (which isn't very helpful - for reference, the unfiltered 2019 dataframe contains 115,090 entries). To address this issue, we only take the first 20 entries for each n=2-5 (i.e. we take the first 20 bigrams, then the first 20 trigrams, etc. for each year).\n",
    "\n",
    "The exception is we take the first 50 monogram entries, since a lot of them tend to be common words and the results are more interesting when we broaden the search. To circumvent this, we also drop the first 10 rows from each dataframe to go further down the monogram list (this can also be adjusted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1734645940563,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "w7FzGJgQ0ht_"
   },
   "outputs": [],
   "source": [
    "# helper function that cleans up the dataframes as outlined above\n",
    "# where df is the ngram dataframe\n",
    "# num_mono is the number of entries to include for monograms\n",
    "# num_other is the number of entries to include for the other ngrams\n",
    "# drop index indicates the number of rows we want to drop from the top of the dataframe\n",
    "def clean_ngram(df, num_mono, num_other, drop_index):\n",
    "  # drop 20 most common ngrams\n",
    "  df = df.iloc[drop_index:]\n",
    "\n",
    "  # filter dataframe by ngram frequency\n",
    "  df1 = df[df['n'] == 1].head(num_mono)\n",
    "  df2 = df[df['n'] == 2].head(num_other)\n",
    "  df3 = df[df['n'] == 3].head(num_other)\n",
    "  df4 = df[df['n'] == 4].head(num_other)\n",
    "  df5 = df[df['n'] == 5].head(num_other)\n",
    "\n",
    "  # concatenate the dataframes along the rows\n",
    "  output_df = pd.concat([df1, df2, df3, df4, df5], axis=0)\n",
    "\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1734645943668,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "TVPwLXRrq8TW"
   },
   "outputs": [],
   "source": [
    "num_entries_mono = 50\n",
    "num_entries_other = 20\n",
    "drop_index = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>you</td>\n",
       "      <td>187</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>was</td>\n",
       "      <td>184</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>181</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>are</td>\n",
       "      <td>159</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>with</td>\n",
       "      <td>152</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>5</td>\n",
       "      <td>incendiary device or technique capable</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>5</td>\n",
       "      <td>firearm explosive or incendiary device</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>5</td>\n",
       "      <td>device or technique capable of</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>5</td>\n",
       "      <td>communities impacted by gun violence</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>5</td>\n",
       "      <td>assembles with one or more</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n                              ngram_text  ngram_count  year\n",
       "10    1                                     you          187  2019\n",
       "11    1                                     was          184  2019\n",
       "12    1                                      on          181  2019\n",
       "13    1                                     are          159  2019\n",
       "14    1                                    with          152  2019\n",
       "...  ..                                     ...          ...   ...\n",
       "2328  5  incendiary device or technique capable            3  2019\n",
       "2442  5  firearm explosive or incendiary device            3  2019\n",
       "2501  5          device or technique capable of            3  2019\n",
       "2541  5    communities impacted by gun violence            3  2019\n",
       "2626  5              assembles with one or more            3  2019\n",
       "\n",
       "[130 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ngram_years_dfs = [clean_ngram(df, num_entries_mono, num_entries_other, drop_index) \n",
    "                         for df in ngram_years_dfs]\n",
    "clean_ngram_years_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1734645962792,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "fhBqJxSt2y0-",
    "outputId": "1183da9d-e575-42fc-f927-cc2ec0281f38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ngram_text</th>\n",
       "      <th>ngram_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>wants</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>soros</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>say</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>omar</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>mueller</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5</td>\n",
       "      <td>vow to remove senior entitlements</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>virginia to outlaw krav maga</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5</td>\n",
       "      <td>ukrainian party girl connected to</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5</td>\n",
       "      <td>ukraine female ukrainian party girl</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>5</td>\n",
       "      <td>tyranny alert virginia to outlaw</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n                           ngram_text  ngram_count  year\n",
       "10   1                                wants            2  2019\n",
       "11   1                                soros            2  2019\n",
       "12   1                                  say            2  2019\n",
       "13   1                                 omar            2  2019\n",
       "14   1                              mueller            2  2019\n",
       "..  ..                                  ...          ...   ...\n",
       "93   5    vow to remove senior entitlements            1  2019\n",
       "98   5         virginia to outlaw krav maga            1  2019\n",
       "109  5    ukrainian party girl connected to            1  2019\n",
       "114  5  ukraine female ukrainian party girl            1  2019\n",
       "119  5     tyranny alert virginia to outlaw            1  2019\n",
       "\n",
       "[130 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ngram_years_headlines_dfs = [clean_ngram(df, num_entries_mono, num_entries_other, drop_index) \n",
    "                                   for df in ngram_years_headlines_dfs]\n",
    "clean_ngram_years_headlines_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBMhLeeD3s9r"
   },
   "source": [
    "We could show the resulting dataframes for the other years as well, but here I've chosen not to in order to save space and improve readability for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJUd0Gyh3rpe"
   },
   "source": [
    "## Write dataframes to excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st2GB47-4AAN"
   },
   "outputs": [],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1734645980648,
     "user": {
      "displayName": "Jodie Lee",
      "userId": "05117498454440747132"
     },
     "user_tz": 480
    },
    "id": "vYC-AUPt4H5-"
   },
   "outputs": [],
   "source": [
    "# create excel writer object to initialize new workbook\n",
    "writer = pd.ExcelWriter(using_dataset.output_path, engine=\"xlsxwriter\")\n",
    "\n",
    "for df in clean_ngram_years_dfs:\n",
    "    year = str(df[\"year\"].iloc[0])\n",
    "    columns_to_save = [col for col in df.columns if col != \"year\"]\n",
    "    df.to_excel(writer, sheet_name=year, columns=columns_to_save, index=False)\n",
    "\n",
    "# close the excel writer and output file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create excel writer object to initialize new workbook\n",
    "writer = pd.ExcelWriter(using_dataset.output_path_headlines, engine=\"xlsxwriter\")\n",
    "\n",
    "for df in clean_ngram_years_headlines_dfs:\n",
    "    year = str(df[\"year\"].iloc[0])\n",
    "    columns_to_save = [col for col in df.columns if col != \"year\"]\n",
    "    df.to_excel(writer, sheet_name=year, columns=columns_to_save, index=False)\n",
    "\n",
    "# close the excel writer and output file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7OonnWjvEWZsKD567Kb1t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
